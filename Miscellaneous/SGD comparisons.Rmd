---
title: "SGD comparisons by Owen (From the class of Fall 2022) (I made some minor adjustments and additions)"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
  
```{r, echo=TRUE}
#Linear regression case ; Data setup
set.seed(1000)
n = 10100; p = 50
beta_real = rnorm(p, sd=5) 
x <- matrix(rnorm((n*p), 0, 1), n, p)
y <- x%*%beta_real + rnorm(n, 0, 1)

itr = 10000 ; tol = 1e-3 ; L = 0.1 ; alpha = 0.0 ; gamma = 0.8

#LS estimate
betahat = solve(t(x)%*%x)%*%t(x)%*%y
mean((beta_real - betahat)^2)
```

## regular gradient descent

```{r, echo=TRUE}
set.seed(1000)
beta = rep(0, p)
Loss = rep(0, itr)

Loss[1] = sum(abs(y-x%*%beta))

for(i in 2:itr){
  betaOld = beta
  derivbeta = -2*t(x)%*%(y-x%*%betaOld)
  beta = betaOld - L * derivbeta
  Loss[i] = sum(abs(y-x%*%beta))
  
  if(abs(Loss[i] - Loss[i-1]) < tol){Loss<- Loss[1:i] ; break}
  
  Counter = 1 ; Lnew = L ; dsqr = sum(derivbeta*derivbeta)
  while (Loss[i] > Loss[i-1] - alpha*Lnew*dsqr){
    Lnew = gamma*Lnew
    beta = betaOld - Lnew * derivbeta
    Loss[i] = sum(abs(y-x%*%beta))
    
    if(Counter > 100){break}
    Counter = Counter + 1
  }
  if(Counter > 100){break}
}

#comparison
mean((beta_real - beta)^2); plot(Loss[1:i])
```

## SGD with randomized rule

```{r, echo=TRUE}
#Validation set to monitor the loss
yval <- y[10001:10100]
xval <- x[10001:10100, ]


y <- y[1:10000]
x <- x[1:10000, ]

set.seed(1000)
beta = rep(0, p)#array(betahat)
Loss = rep(0, itr)

rand = round(runif(1, 1, nrow(x)), 0)
Loss[1] = sum(abs(yval-xval%*%beta))

stopping = F

for(i in 2:itr){
  L = 1/(i)  # notice now this is the step size control.
  
  betaOld = beta
  rand = sample(1:nrow(x), 1)
  derivbeta = -2*array(x[rand,])*as.numeric(y[rand]-x[rand,]%*%betaOld)
  beta = betaOld - L * derivbeta
  
  Loss[i] = sum(abs(yval-xval%*%beta))#(y[rand]-x[rand,]%*%betaOld)^2# #not necessary, but for fun
  
  if(stopping){
   if(i > 500){
   if(abs(Loss[i-1]-Loss[i])/Loss[i-1]<1e-5 || Loss[i-1] < Loss[i]){
    break
   } 
  } 
  }
}


#comparison
mean((beta_real - beta)^2); plot(Loss[1:i])

```

## SGD with mini-batch

```{r, echo=TRUE}
#stochastic grad "descent" - i.e. minimize
set.seed(1000)
beta = rep(0, p)
Loss = rep(0, itr)

#stochastic gradient descent mini-batch - set of values value.
b = 10 # mini-batch size!

#get random SET of data and use it to calculate loss.
ss = round(runif(b, 1, nrow(x)), 0)
Loss[1] = sum(abs(yval-xval%*%beta))

for(i in 2:itr){
  ss = round(runif(b, 1, nrow(x)), 0)
  
  L = 1/i  
  
  betaOld = beta
  derivbeta = -2*t(x[ss,])%*%(y[ss]-x[ss,]%*%betaOld)/b
  beta = betaOld - L * derivbeta

  Loss[i] = sum(abs(yval-xval%*%beta)) #not necessary, but for fun
  
  if(stopping){
   if(i > 500){
   if(abs(Loss[i-1]-Loss[i])/Loss[i-1]<1e-5 || Loss[i-1] < Loss[i]){
    break
   } 
  } 
  }
}

#comparison
mean((beta_real - beta)^2); plot(Loss[1:i], type='l')

```

## SGD with momentum

```{r, echo=TRUE}
#stochastic grad "descent" - i.e. minimize
set.seed(1000)
beta = rep(0, p)
Loss = rep(0, itr)
#stochastic gradient descent with momentum!

#get random sample and use it to calculate loss.
rand = round(runif(1, 1, nrow(x)), 0)
Loss[1] = sum(abs(yval-xval%*%beta))

#momentum and decay
momentum = rep(0, p)
decay = 0.9 #This is the default value in most packages
rate = 0.5 # can control how fast the decay rate reduces 

update = F #Wheather to update the momentum

for(i in 2:itr){
  if(i>2){momentum = beta - betaOld} #update momentum
  
  L = 1/i
  betaOld = beta
  
  rand = sample(1:nrow(x), 1)
  
  derivbeta = -2*array(x[rand,])*as.numeric(y[rand]-x[rand,]%*%betaOld)
  
  if(update){
   if(i %% 10){decay = rate*decay} 
  } # reducing momentum every 10th iteration
  
  beta = betaOld - L * derivbeta + decay * momentum
  
  Loss[i] = sum(abs(yval-xval%*%beta)) #not necessary, but for fun
  
  if(stopping){
   if(i > 500){
   if(abs(Loss[i-1]-Loss[i])/Loss[i]<1e-5 || Loss[i-1] < Loss[i]){
    break
   } 
  } 
  }
}

#comparison
mean((beta_real - beta)^2); plot(Loss[1:i])

```

## SGD with minibatch and momentum

```{r, echo=TRUE}
#stochastic grad "descent" - i.e. minimize
set.seed(1000)
beta = rep(0, p)
Loss = rep(0, itr)

#stochastic gradient descent mini-batch AND momentum.
b = 50 #mini-batch size!
tol = 1e-3 ; L = 0.1 ;  alpha = 0.01 ; gamma = 0.8 

#get random SET of data and use it to calculate loss.
ss = round(runif(b, 1, nrow(x)), 0)
Loss[1] = sum(abs(y[ss]-x[ss,]%*%beta))

#momentum and decay
momentum = rep(0, p)
decay = 0.9
rate = 0.5

update = F #Wheather to update the momentum

for(i in 2:itr){
  if(i>2){momentum = beta - betaOld} #update momentum
  
  ss = sample(1:nrow(x), b)
  L = 1/i  
  
  if(update){
   if(i %% 10){decay = rate*decay} 
  }
  
  betaOld = beta
  derivbeta = -2*t(x[ss,])%*%(y[ss]-x[ss,]%*%betaOld)/b
  beta = betaOld - L * derivbeta + decay * momentum
  
  Loss[i] = sum(abs(yval-xval%*%beta)) #not necessary, but for fun
  
  if(stopping){
   if(i > 500){
   if(abs(Loss[i-1]-Loss[i])/Loss[i]<1e-5 || Loss[i-1] < Loss[i]){
    break
   } 
  } 
  }
}
#comparison
mean((beta_real - beta)^2); plot(Loss[1:i])

```
